- name: 添加本地解析
  become: true
  lineinfile:
    path: /etc/hosts
    state: present
    line: "{{ ansible_default_ipv4.address }} {{ item }}"
  with_items:
    - dss-apiservice-server
    - dss-flow-entrance
    - dss-framework-orchestrator-server-dev
    - dss-framework-project-server
    - dss-guide-server
    - dss-scriptis-server
    - dss-workflow-server-dev
    - exchangis-server
    - linkis-cg-engineconn
    - linkis-cg-engineconnmanager
    - linkis-cg-engineplugin
    - linkis-cg-entrance
    - linkis-cg-linkismanager
    - linkis-mg-eureka
    - linkis-mg-gateway
    - linkis-ps-publicservice
    - streamis-server

- name: 创建linkis程序目录
  become: true
  file:
    path: "{{ item }}"
    state: directory
    owner: hadoop
    group: hadoop
  with_items:
    - "{{ LINKIS_HOME }}"
    - "{{ LINKIS_WEB_DIR }}"
    - /data
    - /home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}
    - /home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}-web

- name: 解压linkis安装包
  shell: "tar zxf {{SOURCE_ROOT_PATH}}/apache-linkis-{{ LINKIS_VERSION }}-bin.tar.gz -C /home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}/"

#- name: 配置权限
#  shell: "chown -R hadoop.hadoop /data /opt/apache-linkis-{{ LINKIS_VERSION }}"

- name: 创建安装配置
  template:
    src: "{{ item }}.j2"
    dest: "/home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}/deploy-config/{{ item }}"
    owner: hadoop
    group: hadoop
  with_items:
    - db.sh
    - linkis-env.sh

- name: 加载环境变量
  shell: "source /etc/profile ~/.bashrc"

- name: 创建hdfs:///linkis
  shell: "sudo -u hdfs hdfs dfs -mkdir -p /linkis ; sudo -u hdfs hdfs dfs -chmod -R 777 /linkis"

- name: 拷贝依赖的jar包
  copy:
    src:  "{{SOURCE_ROOT_PATH}}/mysql-connector-java-5.1.49.jar"
    dest: "/home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}/linkis-package/lib/{{ item }}"
  with_items:
    - linkis-commons/public-module/
    - linkis-spring-cloud-services/linkis-mg-gateway/

- name: 设置软连接
  file:
    src: /home/hadoop/.profile
    dest: "/home/hadoop/.bash_profile"
    state: link

- name: 关闭yum检测
  replace:
    path: "/home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}/bin/checkEnv.sh"
    regexp: "^need_cmd yum"
    replace: "#need_cmd yum"

- name: 修改安装脚本
  replace:
    path: "/home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}/bin/checkEnv.sh"
    regexp: '^   read.*'
    replace: "   idx=1"

- name: 修改安装脚本
  lineinfile:
    dest: "/home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}/bin/install.sh"
    regexp: '^read'
    line: "idx=2"

- name: 安装linkis
  shell:
    chdir: "/home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}"
    cmd: "source /etc/profile ~/.bashrc ; sh bin/install.sh"
  register: install_info

- name: 打印安装信息
  debug: var=install_info.stdout_lines

- name: 删除hive等旧版依赖包
  file:
    path: "{{ item }}"
    state: absent
  with_items:
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hadoop-client-3.1.1.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hadoop-mapreduce-client-common-3.1.1.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hadoop-mapreduce-client-core-3.1.1.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hadoop-mapreduce-client-jobclient-3.1.1.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hadoop-yarn-api-3.1.1.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hadoop-yarn-client-3.1.1.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hadoop-yarn-common-3.1.1.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hadoop-yarn-registry-3.1.0.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hive-classification-3.1.0.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hive-common-3.1.0.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hive-exec-3.1.0.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hive-llap-client-3.1.0.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hive-llap-common-3.1.0.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hive-llap-tez-3.1.0.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hive-storage-api-2.7.0.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hive-upgrade-acid-3.1.0.jar
    - /home/hadoop/linkis/lib/linkis-engineconn-plugins/hive/dist/3.1.0/lib/hive-vector-code-gen-3.1.0.jar

- name: 添加依赖包
  copy:
    owner: hadoop
    group: hadoop
    src: "{{ item }}"
    dest: "{{ LINKIS_HOME }}/lib/linkis-engineconn-plugins/hive/dist/{{HIVE_VERSION}}/lib/"
  with_fileglob:
    - /usr/hdp/current/zookeeper-server/zookeeper-3.4.6.3.1.5.0-152.jar
    - /usr/hdp/current/tez-client/tez-*.jar
    - /usr/hdp/3.1.5.0-152/spark2/jars/hadoop-client-3.1.1.3.1.5.0-152.jar
    - /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-client-*.jar
    - /usr/hdp/current/hadoop-yarn-client/hadoop-yarn*.jar
    - /usr/hdp/current/hive-client/lib/hive*.jar
    - /usr/hdp/current/hadoop-yarn-client/lib/jackson-jaxrs-json-provider-2.10.0.jar
    - /usr/hdp/current/hadoop-yarn-client/lib/jackson-jaxrs-base-2.10.0.jar
    - /usr/hdp/current/hadoop-yarn-client/lib/jackson-module-jaxb-annotations-2.10.0.jar
    - /usr/hdp/3.1.5.0-152/hive/lib/datanucleus-api-jdo-4.2.4.jar
    - /usr/hdp/3.1.5.0-152/hive/lib/datanucleus-rdbms-4.1.19.jar
    - /usr/hdp/3.1.5.0-152/hive/lib/javax.jdo-3.2.0-m3.jar
    - /usr/hdp/3.1.5.0-152/hive/lib/jdo-api-3.0.1.jar

- name: 创建linkis程序目录
  file:
    path: "{{ item }}"
    state: directory
    owner: hadoop
    group: hadoop
  with_items:
    - "{{ HADOOP_CONF_DIR }}"
    - "{{ HIVE_CONF_DIR }}"

- name: 添加hadoop依赖xml
  copy:
    owner: hadoop
    group: hadoop
    src: "{{ item }}"
    dest: "{{HADOOP_CONF_DIR}}"
  with_fileglob:
    - "/usr/hdp/current/hadoop-client/conf/*.xml"

- name: 修改hadoop依赖xml
  shell:
    chdir: "{{ HADOOP_CONF_DIR }}"
    cmd: "{{ item }}"
  with_items:
    - sed -i 's/${hdp.version}/3.1.5.0-152/g' *.xml
    - chown hadoop.hadoop *

- name: 添加hive依赖xml
  copy:
    owner: hadoop
    group: hadoop
    src: "{{ item }}"
    dest: "{{HIVE_CONF_DIR}}"
  with_fileglob:
    - "/usr/hdp/current/hive-client/conf/*.xml"

- name: 修改hive依赖xml
  shell:
    chdir: "{{ HIVE_CONF_DIR }}"
    cmd: "{{ item }}"
  with_items:
    - sed -i 's/${hdp.version}/3.1.5.0-152/g' *.xml
    - chown hadoop.hadoop *

- name: 修改hive配置文件，添加tez
  lineinfile:
    path: "{{ HIVE_CONF_DIR }}/hive-site.xml"
    insertbefore: '^  </configuration>'
    state: present
    line: "{{ item }}"
  with_items:
    - "    <property>"
    - "        <name>tez.lib.uris</name>"
    - "        <value>file:///usr/hdp/current/tez-client/lib/tez.tar.gz</value>"
    - "    </property>"
    - "    <property>"
    - "        <name>hive.tez.container.size</name>"
    - "        <value>1024</value>"
    - "    </property>"

- name: 添加hadoop环境变量
  become: true
  lineinfile:
    dest: /etc/profile
    regexp: "^export HADOOP_CONF_DIR="
    state: present
    line: export HADOOP_CONF_DIR={{HADOOP_CONF_DIR}}

- name: 添加hive环境变量
  become: true
  lineinfile:
    dest: /etc/profile
    regexp: "^export HIVE_CONF_DIR="
    state: present
    line: export HIVE_CONF_DIR={{HIVE_CONF_DIR}}

- name: 开放linkis访问所有权限
  become: true
  command:  mysql --login-path=local --database=linkis -e "{{ item }}"
  with_items:
    - INSERT INTO `linkis_mg_gateway_auth_token`(`token_name`,`legal_users`,`legal_hosts`,`business_owner`,`create_time`,`update_time`,`elapse_day`,`update_by`) VALUES ('QML-AUTH','*','*','BDP',curdate(),curdate(),-1,'LINKIS');
    - INSERT INTO `linkis_mg_gateway_auth_token`(`token_name`,`legal_users`,`legal_hosts`,`business_owner`,`create_time`,`update_time`,`elapse_day`,`update_by`) VALUES ('BML-AUTH','*','*','BDP',curdate(),curdate(),-1,'LINKIS');
    - INSERT INTO `linkis_mg_gateway_auth_token`(`token_name`,`legal_users`,`legal_hosts`,`business_owner`,`create_time`,`update_time`,`elapse_day`,`update_by`) VALUES ('WS-AUTH','*','*','BDP',curdate(),curdate(),-1,'LINKIS');
    - INSERT INTO `linkis_mg_gateway_auth_token`(`token_name`,`legal_users`,`legal_hosts`,`business_owner`,`create_time`,`update_time`,`elapse_day`,`update_by`) VALUES ('dss-AUTH','*','*','BDP',curdate(),curdate(),-1,'LINKIS');
    - INSERT INTO `linkis_mg_gateway_auth_token`(`token_name`,`legal_users`,`legal_hosts`,`business_owner`,`create_time`,`update_time`,`elapse_day`,`update_by`) VALUES ('QUALITIS-AUTH','*','*','BDP',curdate(),curdate(),-1,'LINKIS');
    - INSERT INTO `linkis_mg_gateway_auth_token`(`token_name`,`legal_users`,`legal_hosts`,`business_owner`,`create_time`,`update_time`,`elapse_day`,`update_by`) VALUES ('VALIDATOR-AUTH','*','*','BDP',curdate(),curdate(),-1,'LINKIS');
    - INSERT INTO `linkis_mg_gateway_auth_token`(`token_name`,`legal_users`,`legal_hosts`,`business_owner`,`create_time`,`update_time`,`elapse_day`,`update_by`) VALUES ('LINKISCLI-AUTH','*','*','BDP',curdate(),curdate(),-1,'LINKIS');
    - INSERT INTO `linkis_mg_gateway_auth_token`(`token_name`,`legal_users`,`legal_hosts`,`business_owner`,`create_time`,`update_time`,`elapse_day`,`update_by`) VALUES ('DSM-AUTH','*','*','BDP',curdate(),curdate(),-1,'LINKIS');
    - INSERT INTO `linkis_mg_gateway_auth_token`(`token_name`,`legal_users`,`legal_hosts`,`business_owner`,`create_time`,`update_time`,`elapse_day`,`update_by`) VALUES ('LINKIS_CLI_TEST','*','*','BDP',curdate(),curdate(),-1,'LINKIS');

- name: 启动linkis服务
  shell:
    chdir: "{{ LINKIS_HOME }}"
    cmd: "source /etc/profile ~/.bashrc ; sh sbin/linkis-start-all.sh"
  register: start_info
  tags:
    - restart

- name: 打印linkis启动信息
  debug: var=start_info.stdout_lines
  tags:
    - restart

- name: 检查端口是否运行
  wait_for: port={{ item }} state=started delay=1 timeout=30
  with_items:
    - 20303   # LINKIS-MG-EUREKA
    - 9001    # LINKIS-MG-GATEWAY
    - 9101    # LINKIS-CG-LINKISMANAGER
    - 9102    # LINKIS-CG-ENGINECONNMANAGER
    - 9104    # LINKIS-CG-ENTRANCE
    - 9105    # LINKIS-PS-PUBLICSERVICE

### 安装linkis前端 ###
- name: 解压linkis前端包
  shell: "tar zxf {{SOURCE_ROOT_PATH}}/apache-linkis-{{ LINKIS_VERSION }}-web-bin.tar.gz -C /home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}-web/"

- name: 创建linkis前端目录
  file:
    path: "{{ LINKIS_WEB_DIR }}"
    state: directory
    owner: hadoop
    group: hadoop

- name: 复制前端代码
  shell:
    chdir: /home/hadoop/datasphere_install/apache-linkis-{{ LINKIS_VERSION }}-web
    cmd: "cp -r dist/* {{ LINKIS_WEB_DIR }}/"

- name: 创建nginx配置
  become: true
  template:
    src: "linkis.conf.j2"
    dest: /etc/nginx/conf.d/linkis.conf

- name: 重载nginx配置
  become: true
  shell: nginx -s reload

- name: 检查端口是否运行
  wait_for: port={{ LINKIS_WEB_PORT }} state=started delay=1 timeout=30
